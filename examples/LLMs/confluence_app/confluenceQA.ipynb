{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Confluence QA App on Neopilot\n",
    "We're revolutionizing chatbot interactions with advancements in AI and NLP like OpenAI's GPT  and LangChain. In this post, we'll explore how to use Neopilot to simplify and enhance the process of building a Q&A app for an Internal Knowledge base from conceptualization to deployment (monitoring and iteration of your application.) (not doing this)\n",
    "\n",
    "GitHub link for [code](Add link)\n",
    "\n",
    "## Solution Overview\n",
    "\n",
    "Answering from a large text is difficult because the models from OpenAI take limited context. There are multiple ways to get around this problem.\n",
    "\n",
    "1. Form text snippets and sequentially query the LLM with a prompt based on the current snippet and refine the answer from previous snippets. It helps in iteratively covering all the text. But this method is slow and cost-ineffective.\n",
    "2. We can use LLMs with longer context windows. There are constant advancements in the field of LLM. Anthropic released a 100k context window Claude model. This is only a partial solution when we query a considerable knowledge base.\n",
    "3. Form a prompt using the nearest text snippets related to the question with the help of embeddings, and query the LLM with the prompt. The idea is to have an embedding vector store for each text snippet. When a question is asked, we compute the embedding of the question and retrieve the nearest embeddings to the question vector using a similarity search. \n",
    "\n",
    "##Architecture:\n",
    "\n",
    "\n",
    "\n",
    "* Step 1:\n",
    "[Knowledge Base] → [Text Snippets] → [Snippet embedding]\n",
    "\n",
    "* Step 2:\n",
    "[User asks a question] → [Compute embedding] → [find relevant snippets using similarity search]\n",
    "\n",
    "* Step 3:\n",
    "[Prompt engineering] using relevant snippets, User's question  -> [Query LLM with Prompt] - Get the answer\n",
    "\n",
    "\n",
    "\n",
    "### Text embeddings:\n",
    "\n",
    "We can use Open source models like SBERT, Universal Sentence Encoder, Instructor-XL, or  OpenAI APIs like text-embedding-ada-002. [MTEB Leader board](https://huggingface.co/spaces/mteb/leaderboard) from Hugging Face compares different models on various tasks. In this work, we use OpenAI's `adav2`.\n",
    "\n",
    "### LLMs:  \n",
    "\n",
    "We can use open-source models like FastChat, Falcon, Flan-T5 or APIs from OpenAI [GPT-3.5 models](https://platform.openai.com/docs/models/gpt-3-5). In this work, we use OpenAI's `gpt-3.5-turbo`\n",
    "\n",
    "* **Step 1: Creating an Embedding Store from the knowledge base:**\n",
    "\n",
    "    In our case, we are using Confluence pages as the knowledge base. Langchain provides a variety of Document Loaders for different knowledge bases like `ConfluenceLoader`, `PDFLoader`, `NotionLoader`.\n",
    "    We use Langchain's `ConfluenceLoader`  with `TextSplitter` and `TokenSplitter` to efficiently split the documents into text snippets. Finally, We create embeddings with OpenAI's `adav2` and store them with Chromadb.\n",
    "\n",
    "    There are many vector stores integration provided by Langchain. We have used `Chromadb` since it was easy to setup. We can design weekly Jobs to extract new Confluence pages and update the Vector DB store. Find the code and relevant description at [Step 1](#step1-creating-an-embedding-store-from-the-knowledge-base)\n",
    "\n",
    "* **Step 2: Computing questions embeddings and finding relevant snippets**\n",
    "\n",
    "    We have used RetreivalQA from Langchain, with ChromaDB to retrieve top K relevant text snippets based on the similarity with questions embedding in [Step 2](#step-2-computing-questions-embeddings-and-finding-relevant-snippets)\n",
    "\n",
    "\n",
    "* **Step 3: Prompt engineering and querying LLM**\n",
    "\n",
    "    We have used the default prompt of RetreivalQA from Langchain. How to add a Custom prompt is shown in [Step 3](#step-3-prompt-engineering-and-querying-llm). \n",
    "\n",
    "\n",
    "* **Step 4: Streamlit App and Creating a Service with Neopilot**\n",
    "\n",
    "    The final step is to package everything into a streamlit application and expose the endpoint, shown in [Step 4](#step-4-streamlit-service-and-creating-a-service-with-neopilot)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize OpenAI Keys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] =\"sk-**\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "EMB_OPENAI_ADA = \"text-embedding-ada-002\"\n",
    "EMB_SBERT = None # Chroma takes care\n",
    "\n",
    "LLM_OPENAI_GPT35 = \"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Installations \n",
    "!pip install langchain==0.0.189\n",
    "!pip install chromadb==0.3.25\n",
    "!pip install openai==0.27.6\n",
    "!pip install pytesseract==0.3.10\n",
    "!pip install beautifulsoup4==4.12.2\n",
    "!pip install atlassian-python-api==3.38.0\n",
    "!pip install tiktoken==0.4.0\n",
    "!pip install lxml==4.9.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import ConfluenceLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter, TokenTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConfluenceQA Initialize\n",
    "\n",
    "**Initialize the embedding model to be used for embedding the text snippets.**\n",
    "\n",
    "* OpenAI provides several embedding models like `ada-v2`, `ada-v1`, `davinci-v1`, `curie-v1`, `babbage-v1`. The default model is `ada-v2` (`text-embedding-ada-002`) which is the most performative and cost-effective. You can learn more about embeddings from [OpenAI Documentation](https://platform.openai.com/docs/guides/embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = OpenAIEmbeddings()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Intialize the LLM model to be used for the final LLM call to query with prompt**\n",
    "\n",
    "* Available OpenAI LLM APIs are \n",
    "    * GPT-4 Models - most powerful and in Limited Beta\n",
    "    * GPT-3.5 Models - Has a context length of 4096 tokens and more powerful than GPT-3 models\n",
    "    * GPT-3 Models - Has a context length of 2049 tokens and are available for finetuning\n",
    "* We have used ChatGPT Model (`gpt-3.5-turbo`), since it's cheapest among GPT-3.5 models. It's advised to try out different models, since some models excel in specific tasks. You can find more about LLM APIs from [OpenAI Documentation](https://platform.openai.com/docs/models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name=LLM_OPENAI_GPT35, temperature=0.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step1: Creating an Embedding Store from the knowledge base:\n",
    "\n",
    "#### Extract the documents with ConfluenceLoader \n",
    "\n",
    "`ConfluenceLoader` can extract the documents with `username`,`apikey` and `confluenceurl`. [ConfluenceLoader](https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/confluence.html?highlight=confluence%20loader) currently supports `username/api_key`, `Oauth2 login` authentication.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"persist_directory\":\"./chroma_db/\",\n",
    "          \"confluence_url\":\"https://templates.atlassian.net/wiki/\",\n",
    "          \"username\":None,\n",
    "          \"api_key\":None,\n",
    "          \"space_key\":\"RD\"\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = config.get(\"persist_directory\",None)\n",
    "confluence_url = config.get(\"confluence_url\",None)\n",
    "username = config.get(\"username\",None)\n",
    "api_key = config.get(\"api_key\",None)\n",
    "space_key = config.get(\"space_key\",None)\n",
    "\n",
    "## 1. Extract the documents\n",
    "loader = ConfluenceLoader(\n",
    "    url=confluence_url,\n",
    "    username = username,\n",
    "    api_key= api_key\n",
    ")\n",
    "documents = loader.load(\n",
    "    space_key=space_key,\n",
    "    limit=100\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Split documents and create text snippets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "text_splitter = TokenTextSplitter(chunk_size=1000, chunk_overlap=10, encoding_name=\"cl100k_base\")  # This the encoding for text-embedding-ada-002\n",
    "texts = text_splitter.split_documents(texts)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Embeddings and add to chroma store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if persist_directory and os.path.exists(persist_directory):\n",
    "    vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)\n",
    "else:\n",
    "    vectordb = Chroma.from_documents(documents=texts, embedding=embedding, persist_directory=persist_directory)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Computing questions embeddings and finding relevant snippets\n",
    "#### Retreival QA Chain\n",
    "<!-- TODO: Add about Retreival QA Chain -->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Step 3: Prompt engineering and  querying LLM\n",
    "* We have used the default prompt from Langchain here\n",
    "    ```python\n",
    "    prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "    {context}\n",
    "\n",
    "    Question: {question}\n",
    "    Helpful Answer:\"\"\"\n",
    "    PROMPT = PromptTemplate(\n",
    "        template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "    ```\n",
    "\n",
    "\n",
    "* For passing a custom prompt with `context` and `question`:\n",
    "    ```python\n",
    "    custom_prompt_template = \"\"\"You are a Confluence chatbot answering questions. Use the following pieces of context to answer the question at the end. If you don't know the answer, say that you don't know, don't try to make up an answer.\n",
    "\n",
    "    {context}\n",
    "\n",
    "    Question: {question}\n",
    "    Helpful Answer:\"\"\"\n",
    "    CUSTOMPROMPT = PromptTemplate(\n",
    "        template=custom_prompt_template, input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "    ## Inject custom prompt \n",
    "    qa.combine_documents_chain.llm_chain.prompt = CUSTOMPROMPT\n",
    "    ```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever(search_kwargs={\"k\":4})\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\",retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To organize content in a space, you can create pages or blogs for different types of content. Pages can have child pages, which allows you to organize content into categories and subcategories. You can also use labels to categorize and identify content, and create a table of contents for your space using the Content Report Table Macro. Additionally, you can customize the sidebar to make it easier to navigate through your space and add a search box to find content within your space.\n"
     ]
    }
   ],
   "source": [
    "question = \"How to organize content in a space?\"\n",
    "\n",
    "answer = qa.run(question)\n",
    "print(answer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's pack every thing into a class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfluenceQA:\n",
    "    def __init__(self,config:dict = {}):\n",
    "        self.config = config\n",
    "        self.embedding = None\n",
    "        self.vectordb = None\n",
    "        self.llm = None\n",
    "        self.qa = None\n",
    "        self.retriever = None\n",
    "    def init_embeddings(self) -> None:\n",
    "        # OpenAI ada embeddings API\n",
    "        self.embedding = OpenAIEmbeddings()\n",
    "    def init_models(self) -> None:\n",
    "        # OpenAI GPT 3.5 API\n",
    "        self.llm = ChatOpenAI(model_name=LLM_OPENAI_GPT35, temperature=0.)\n",
    "        \n",
    "    def vector_db_confluence_docs(self,force_reload:bool= False) -> None:\n",
    "        \"\"\"\n",
    "        creates vector db for the embeddings and persists them or loads a vector db from the persist directory\n",
    "        \"\"\"\n",
    "        persist_directory = self.config.get(\"persist_directory\",None)\n",
    "        confluence_url = self.config.get(\"confluence_url\",None)\n",
    "        username = self.config.get(\"username\",None)\n",
    "        api_key = self.config.get(\"api_key\",None)\n",
    "        space_key = self.config.get(\"space_key\",None)\n",
    "        if persist_directory and os.path.exists(persist_directory) and not force_reload:\n",
    "            ## Load from the persist db\n",
    "            self.vectordb = Chroma(persist_directory=persist_directory, embedding_function=self.embedding)\n",
    "        else:\n",
    "            ## 1. Extract the documents\n",
    "            loader = ConfluenceLoader(\n",
    "                url=confluence_url,\n",
    "                username = username,\n",
    "                api_key= api_key\n",
    "            )\n",
    "            documents = loader.load(\n",
    "                space_key=space_key, \n",
    "                limit=100)\n",
    "            ## 2. Split the texts\n",
    "            text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=0)\n",
    "            texts = text_splitter.split_documents(documents)\n",
    "            text_splitter = TokenTextSplitter(chunk_size=1000, chunk_overlap=10, encoding_name=\"cl100k_base\")  # This the encoding for text-embedding-ada-002\n",
    "            texts = text_splitter.split_documents(texts)\n",
    "\n",
    "            ## 3. Create Embeddings and add to chroma store\n",
    "            ##TODO: Validate if self.embedding is not None\n",
    "            self.vectordb = Chroma.from_documents(documents=texts, embedding=self.embedding, persist_directory=persist_directory)\n",
    "    def retreival_qa_chain(self):\n",
    "        \"\"\"\n",
    "        Creates retrieval qa chain using vectordb as retrivar and LLM to complete the prompt\n",
    "        \"\"\"\n",
    "        ##TODO: Use custom prompt\n",
    "        self.retriever = self.vectordb.as_retriever(search_kwargs={\"k\":4})\n",
    "        self.qa = RetrievalQA.from_chain_type(llm=self.llm, chain_type=\"stuff\",retriever=self.retriever)\n",
    "\n",
    "    def answer_confluence(self,question:str) ->str:\n",
    "        \"\"\"\n",
    "        Answer the question\n",
    "        \"\"\"\n",
    "        answer = self.qa.run(question)\n",
    "        return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"persist_directory\":\"./chroma_db/\",\n",
    "          \"confluence_url\":\"https://templates.atlassian.net/wiki/\",\n",
    "          \"username\":None,\n",
    "          \"api_key\":None,\n",
    "          \"space_key\":\"RD\"}\n",
    "confluenceQA = ConfluenceQA(config=config)\n",
    "confluenceQA.init_embeddings()\n",
    "confluenceQA.init_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Vector DB \n",
    "\n",
    "confluenceQA.vector_db_confluence_docs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Retreival QA Chain\n",
    "confluenceQA.retreival_qa_chain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run the chain\n",
    "question = \"How to organize content in a space?\"\n",
    "confluenceQA.answer_confluence(question)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Streamlit service and Creating a service with Neopilot"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can have a streamlit app around this and create a service to deploy locally or on cluster.\n",
    "* Neopilot helps to productionise the service quickly\n",
    "* Sabrina can help to add more on deployment part here\n",
    "\n",
    "```python\n",
    "import streamlit as st\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from confluence_qa import ConfluenceQA\n",
    "load_dotenv()\n",
    "st.set_page_config(\n",
    "    page_title='Q&A Bot for Confluence Page',\n",
    "    page_icon='⚡',\n",
    "    layout='wide',\n",
    "    initial_sidebar_state='auto',\n",
    ")\n",
    "\n",
    "st.session_state[\"config\"] = {}\n",
    "confluence_qa = None  # Define confluence_qa initially as None\n",
    "\n",
    "@st.cache_resource\n",
    "def load_confluence(config):\n",
    "    # st.write(\"loading the confluence page\")\n",
    "    confluence_qa = ConfluenceQA(config=config)\n",
    "    confluence_qa.init_embeddings()\n",
    "    confluence_qa.init_models()\n",
    "    confluence_qa.vector_db_confluence_docs()\n",
    "    confluence_qa.retreival_qa_chain()\n",
    "    return confluence_qa\n",
    "\n",
    "with st.sidebar.form(key ='Form1'):\n",
    "    st.markdown('## Add your configs')\n",
    "    confluence_url = st.text_input(\"paste the confluence URL\", \"https://templates.atlassian.net/wiki/\")\n",
    "    username = st.text_input(label=\"confluence username\",\n",
    "                             help=\"leave blank if confluence page is public\",\n",
    "                             type=\"password\")\n",
    "    space_key = st.text_input(label=\"confluence space\",\n",
    "                             help=\"Space of Confluence\",\n",
    "                             value=\"RD\")\n",
    "    api_key = st.text_input(label=\"confluence api key\",\n",
    "                            help=\"leave blank if confluence page is public\",\n",
    "                            type=\"password\")\n",
    "    submitted1 = st.form_submit_button(label='Submit')\n",
    "\n",
    "    if submitted1 and confluence_url and space_key:\n",
    "        st.session_state[\"config\"] = {\n",
    "            \"persist_directory\": None,\n",
    "            \"confluence_url\": confluence_url,\n",
    "            \"username\": username if username != \"\" else None,\n",
    "            \"api_key\": api_key if api_key != \"\" else None,\n",
    "            \"space_key\": space_key,\n",
    "        }\n",
    "        with st.spinner(text=\"Ingesting Confluence...\"):\n",
    "            confluence_qa = load_confluence(st.session_state[\"config\"])\n",
    "            st.session_state[\"confluence_qa\"] = confluence_qa\n",
    "        st.write(\"Confluence Space Ingested\")\n",
    "        \n",
    "\n",
    "st.title(\"Confluence Q&A Demo\")\n",
    "\n",
    "question = st.text_input('Ask a question', \"How do I make a space public?\")\n",
    "\n",
    "if st.button('Get Answer', key='button2'):\n",
    "    with st.spinner(text=\"Asking LLM...\"):\n",
    "        confluence_qa = st.session_state.get(\"confluence_qa\")\n",
    "        if confluence_qa is not None:\n",
    "            result = confluence_qa.answer_confluence(question)\n",
    "            st.write(result)\n",
    "        else:\n",
    "            st.write(\"Please load Confluence page first.\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
