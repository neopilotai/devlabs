{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "225ea87f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ray version 1.2.0\n",
      "pandas version 1.1.4\n",
      "raydp version 0.1.1\n",
      "pyspark version 3.0.3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ray\n",
    "import raydp\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "\n",
    "print(f'ray version {ray.__version__}')\n",
    "print(f'pandas version {pd.__version__}')\n",
    "print(f'raydp version {raydp.__version__}')\n",
    "print(f'pyspark version {pyspark.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a09067d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk 11.0.13 2021-10-19\n",
      "OpenJDK Runtime Environment (build 11.0.13+8-Ubuntu-0ubuntu1.20.04)\n",
      "OpenJDK 64-Bit Server VM (build 11.0.13+8-Ubuntu-0ubuntu1.20.04, mixed mode, sharing)\n"
     ]
    }
   ],
   "source": [
    "!java --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7aabce1",
   "metadata": {},
   "source": [
    "### start ray cluster, since we are on the head node, use default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d21fd3b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-27 21:55:31,309\tWARNING utils.py:479 -- Detecting docker specified CPUs. In previous versions of Ray, CPU detection in containers was incorrect. Please ensure that Ray has enough CPUs allocated. As a temporary workaround to revert to the prior behavior, set `RAY_USE_MULTIPROCESSING_CPU_COUNT=1` as an env var before starting Ray. Set the env var: `RAY_DISABLE_DOCKER_CPU_WARNING=1` to mute this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleting pod ray-worker-2f43d397-d6d4-4149-ace5-2dec9f63bc51\n",
      "deleting pod ray-worker-3ce5b2e4-22ca-4750-8385-a6fba39cdf64\n",
      "ðŸ‘‰ Hyperplane: selecting worker node pool\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-27 21:55:31,771\tINFO services.py:1172 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://10.1.88.2:8787\u001b[39m\u001b[22m\n",
      "2021-12-27 21:55:31,775\tWARNING services.py:1619 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67108864 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=Xgb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 2gb.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ray dashboard available at https://shakdemo.hyperplane.dev/ray-stella2/#/\n",
      "Waiting for worker ray-worker-34a60559-da51-49eb-9c36-d58267921fec...\n",
      "Waiting for worker ray-worker-339ce231-fa5d-43f9-aa16-6d229ae467bb...\n"
     ]
    }
   ],
   "source": [
    "from hyperplane.ray_common import initialize_ray_cluster, stop_ray_cluster, find_ray_workers\n",
    "num_workers = 2\n",
    "cpu_core_per_worker = 15\n",
    "ram_gb_per_worker = 12 #110 GB allocatible for 16_128 nodes, 12 for 16_16 nodes, 27 for 32_32 nodes\n",
    "ray_cluster = initialize_ray_cluster(num_workers, cpu_core_per_worker, ram_gb_per_worker)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbea4fa",
   "metadata": {},
   "source": [
    "### change the logging level of spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "986e608f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/conda/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.0.3.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "21/12/27 21:55:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/12/27 21:56:18 WARN HttpParser: Header is too large 8193>8192\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "conf = SparkConf().set('spark.ui.port', '8788')\n",
    "sc = SparkContext(conf=conf)\n",
    "log4j = sc._jvm.org.apache.log4j\n",
    "log4j.LogManager.getRootLogger().setLevel(log4j.Level.ERROR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8c8329",
   "metadata": {},
   "source": [
    "### start spark session "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7dcf171",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/opt/conda/lib/python3.8/site-packages/ray/jars/ray_dist.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/opt/conda/lib/python3.8/site-packages/pyspark/jars/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/conda/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.0.3.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-27 21:57:46 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = raydp.init_spark('example', num_executors=2, executor_cores=4, executor_memory='4G')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae82bd3",
   "metadata": {},
   "source": [
    "### read tsv data from s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2dc4adca",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = spark.read.csv(path='s3a://d2v-tmp/demo/bach_inference/data/imdb_reviews.tsv', sep ='\\t', header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9933435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+--------------------+\n",
      "|     id|sentiment|              review|\n",
      "+-------+---------+--------------------+\n",
      "| 5814_8|        1|With all this stu...|\n",
      "| 2381_9|        1|\"The Classic War ...|\n",
      "| 7759_3|        0|The film starts w...|\n",
      "| 3630_4|        0|It must be assume...|\n",
      "| 9495_8|        1|Superbly trashy a...|\n",
      "| 8196_8|        1|I dont know why p...|\n",
      "| 7166_2|        0|This movie could ...|\n",
      "|10633_1|        0|I watched this vi...|\n",
      "|  319_1|        0|A friend of mine ...|\n",
      "|8713_10|        1|<br /><br />This ...|\n",
      "| 2486_3|        0|What happens when...|\n",
      "|6811_10|        1|Although I genera...|\n",
      "|11744_9|        1|\"Mr. Harvey Light...|\n",
      "| 7369_1|        0|I had a feeling t...|\n",
      "|12081_1|        0|note to George Li...|\n",
      "| 3561_4|        0|Stephen King adap...|\n",
      "| 4489_1|        0|`The Matrix' was ...|\n",
      "| 3951_2|        0|Ulli Lommel's 198...|\n",
      "|3304_10|        1|This movie is one...|\n",
      "|9352_10|        1|Most people, espe...|\n",
      "+-------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3634d6d6",
   "metadata": {},
   "source": [
    "### do some cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2118497",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## dropna\n",
    "ds = ds.dropna()\n",
    "ds.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0da306d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+--------------------+--------------------+\n",
      "|    id|sentiment|              review|        review_clean|\n",
      "+------+---------+--------------------+--------------------+\n",
      "|5814_8|        1|With all this stu...|With all this stu...|\n",
      "|2381_9|        1|\"The Classic War ...|\"The Classic War ...|\n",
      "|7759_3|        0|The film starts w...|The film starts w...|\n",
      "|3630_4|        0|It must be assume...|It must be assume...|\n",
      "|9495_8|        1|Superbly trashy a...|Superbly trashy a...|\n",
      "+------+---------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## remove html tags\n",
    "from pyspark.sql.functions import col, udf,regexp_replace,isnull\n",
    "ds = ds.withColumn(\"review_clean\",regexp_replace(col('review'), '<[^>]+>', ''))\n",
    "ds.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5145e8",
   "metadata": {},
   "source": [
    "### save cleaned data to parquet on s3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36adde27",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ds.write.parquet(\"s3a://d2v-tmp/demo/bach_inference/data/imdb_reviews_clean.parquet\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb356c4",
   "metadata": {},
   "source": [
    "### read back parquet data with pandas to do downstream tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca12ae95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "      <th>review_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5814_8</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2381_9</td>\n",
       "      <td>1</td>\n",
       "      <td>\"The Classic War of the Worlds\" by Timothy Hin...</td>\n",
       "      <td>\"The Classic War of the Worlds\" by Timothy Hin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id sentiment                                             review  \\\n",
       "0  5814_8         1  With all this stuff going down at the moment w...   \n",
       "1  2381_9         1  \"The Classic War of the Worlds\" by Timothy Hin...   \n",
       "\n",
       "                                        review_clean  \n",
       "0  With all this stuff going down at the moment w...  \n",
       "1  \"The Classic War of the Worlds\" by Timothy Hin...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet(\"s3://d2v-tmp/demo/bach_inference/data/imdb_reviews_clean.parquet\")\n",
    "print(df.shape)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d66c027f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting ray-worker-2f43d397-d6d4-4149-ace5-2dec9f63bc51\n",
      "Deleting ray-worker-3ce5b2e4-22ca-4750-8385-a6fba39cdf64\n"
     ]
    }
   ],
   "source": [
    "stop_ray_cluster(ray_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "962e2e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ray-worker-2f43d397-d6d4-4149-ace5-2dec9f63bc51\tRunning\t10.1.91.3\n",
      "ray-worker-3ce5b2e4-22ca-4750-8385-a6fba39cdf64\tRunning\t10.1.92.3\n"
     ]
    }
   ],
   "source": [
    "#Use this in case you forgot your workers\n",
    "w = find_ray_workers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e093fd45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
