{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0aa5db0a",
   "metadata": {},
   "source": [
    "## Convert tensorflow model to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360bb77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    " \n",
    "!{sys.executable} -m pip install --quiet --upgrade onnxruntime==1.4.0\n",
    "!{sys.executable} -m pip install --quiet --upgrade onnxruntime-tools==1.4.0\n",
    "!{sys.executable} -m pip install --quiet --upgrade transformers\n",
    "\n",
    "import tf2onnx; print(tf2onnx.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15347788",
   "metadata": {},
   "source": [
    "## Convert to onnx through transformers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4e85df3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Building PyTorch model from configuration: BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Converting TensorFlow checkpoint from /root/ds-29faf59/starter_notebooks/triton/torch_model/bert_model.ckpt\n",
      "Loading TF weight bert/embeddings/LayerNorm/beta with shape [512]\n",
      "Loading TF weight bert/embeddings/LayerNorm/gamma with shape [512]\n",
      "Loading TF weight bert/embeddings/position_embeddings with shape [512, 512]\n",
      "Loading TF weight bert/embeddings/token_type_embeddings with shape [2, 512]\n",
      "Loading TF weight bert/embeddings/word_embeddings with shape [30522, 512]\n",
      "2021-08-27 22:11:17.066930: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 62509056 exceeds 10% of free system memory.\n",
      "Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/beta with shape [512]\n",
      "Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/gamma with shape [512]\n",
      "Loading TF weight bert/encoder/layer_0/attention/output/dense/bias with shape [512]\n",
      "Loading TF weight bert/encoder/layer_0/attention/output/dense/kernel with shape [512, 512]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/key/bias with shape [512]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/key/kernel with shape [512, 512]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/query/bias with shape [512]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/query/kernel with shape [512, 512]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/value/bias with shape [512]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/value/kernel with shape [512, 512]\n",
      "Loading TF weight bert/encoder/layer_0/intermediate/dense/bias with shape [2048]\n",
      "Loading TF weight bert/encoder/layer_0/intermediate/dense/kernel with shape [512, 2048]\n",
      "Loading TF weight bert/encoder/layer_0/output/LayerNorm/beta with shape [512]\n",
      "Loading TF weight bert/encoder/layer_0/output/LayerNorm/gamma with shape [512]\n",
      "Loading TF weight bert/encoder/layer_0/output/dense/bias with shape [512]\n",
      "Loading TF weight bert/encoder/layer_0/output/dense/kernel with shape [2048, 512]\n",
      "Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/beta with shape [512]\n",
      "Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/gamma with shape [512]\n",
      "Loading TF weight bert/encoder/layer_1/attention/output/dense/bias with shape [512]\n",
      "Loading TF weight bert/encoder/layer_1/attention/output/dense/kernel with shape [512, 512]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/key/bias with shape [512]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/key/kernel with shape [512, 512]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/query/bias with shape [512]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/query/kernel with shape [512, 512]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/value/bias with shape [512]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/value/kernel with shape [512, 512]\n",
      "Loading TF weight bert/encoder/layer_1/intermediate/dense/bias with shape [2048]\n",
      "Loading TF weight bert/encoder/layer_1/intermediate/dense/kernel with shape [512, 2048]\n",
      "Loading TF weight bert/encoder/layer_1/output/LayerNorm/beta with shape [512]\n",
      "Loading TF weight bert/encoder/layer_1/output/LayerNorm/gamma with shape [512]\n",
      "Loading TF weight bert/encoder/layer_1/output/dense/bias with shape [512]\n",
      "Loading TF weight bert/encoder/layer_1/output/dense/kernel with shape [2048, 512]\n",
      "Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/beta with shape [512]\n",
      "Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/gamma with shape [512]\n",
      "Loading TF weight bert/encoder/layer_2/attention/output/dense/bias with shape [512]\n",
      "Loading TF weight bert/encoder/layer_2/attention/output/dense/kernel with shape [512, 512]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/key/bias with shape [512]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/key/kernel with shape [512, 512]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/query/bias with shape [512]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/query/kernel with shape [512, 512]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/value/bias with shape [512]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/value/kernel with shape [512, 512]\n",
      "Loading TF weight bert/encoder/layer_2/intermediate/dense/bias with shape [2048]\n",
      "Loading TF weight bert/encoder/layer_2/intermediate/dense/kernel with shape [512, 2048]\n",
      "Loading TF weight bert/encoder/layer_2/output/LayerNorm/beta with shape [512]\n",
      "Loading TF weight bert/encoder/layer_2/output/LayerNorm/gamma with shape [512]\n",
      "Loading TF weight bert/encoder/layer_2/output/dense/bias with shape [512]\n",
      "Loading TF weight bert/encoder/layer_2/output/dense/kernel with shape [2048, 512]\n",
      "Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/beta with shape [512]\n",
      "Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/gamma with shape [512]\n",
      "Loading TF weight bert/encoder/layer_3/attention/output/dense/bias with shape [512]\n",
      "Loading TF weight bert/encoder/layer_3/attention/output/dense/kernel with shape [512, 512]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/key/bias with shape [512]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/key/kernel with shape [512, 512]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/query/bias with shape [512]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/query/kernel with shape [512, 512]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/value/bias with shape [512]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/value/kernel with shape [512, 512]\n",
      "Loading TF weight bert/encoder/layer_3/intermediate/dense/bias with shape [2048]\n",
      "Loading TF weight bert/encoder/layer_3/intermediate/dense/kernel with shape [512, 2048]\n",
      "Loading TF weight bert/encoder/layer_3/output/LayerNorm/beta with shape [512]\n",
      "Loading TF weight bert/encoder/layer_3/output/LayerNorm/gamma with shape [512]\n",
      "Loading TF weight bert/encoder/layer_3/output/dense/bias with shape [512]\n",
      "Loading TF weight bert/encoder/layer_3/output/dense/kernel with shape [2048, 512]\n",
      "Loading TF weight bert/pooler/dense/bias with shape [512]\n",
      "Loading TF weight bert/pooler/dense/kernel with shape [512, 512]\n",
      "Loading TF weight cls/predictions/output_bias with shape [30522]\n",
      "Loading TF weight cls/predictions/transform/LayerNorm/beta with shape [512]\n",
      "Loading TF weight cls/predictions/transform/LayerNorm/gamma with shape [512]\n",
      "Loading TF weight cls/predictions/transform/dense/bias with shape [512]\n",
      "Loading TF weight cls/predictions/transform/dense/kernel with shape [512, 512]\n",
      "Loading TF weight cls/seq_relationship/output_bias with shape [2]\n",
      "Loading TF weight cls/seq_relationship/output_weights with shape [2, 512]\n",
      "Initialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'embeddings', 'position_embeddings']\n",
      "Initialize PyTorch weight ['bert', 'embeddings', 'token_type_embeddings']\n",
      "Initialize PyTorch weight ['bert', 'embeddings', 'word_embeddings']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'pooler', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'pooler', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['cls', 'predictions', 'output_bias']\n",
      "Initialize PyTorch weight ['cls', 'predictions', 'transform', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['cls', 'predictions', 'transform', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['cls', 'predictions', 'transform', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['cls', 'predictions', 'transform', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['cls', 'seq_relationship', 'output_bias']\n",
      "Initialize PyTorch weight ['cls', 'seq_relationship', 'output_weights']\n",
      "Save PyTorch model to ./torch_model/pytorch_model.bin\n",
      "CPU times: user 144 ms, sys: 50.5 ms, total: 194 ms\n",
      "Wall time: 7.36 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "!transformers-cli convert --model_type bert \\\n",
    "  --tf_checkpoint ./torch_model/bert_model.ckpt \\\n",
    "  --config ./torch_model/bert_config.json \\\n",
    "  --pytorch_dump_output ./torch_model/pytorch_model.bin"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
