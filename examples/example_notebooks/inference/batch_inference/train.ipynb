{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "118dc304",
   "metadata": {},
   "source": [
    "## fine-tune tensorflow distilBert and save model to cloud for usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53c4fc3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "detectron2 0.5+cu110 requires black==21.4b2, but you have black 21.9b0 which is incompatible.\n",
      "prefect 0.15.6 requires requests<2.26,>=2.20, but you have requests 2.26.0 which is incompatible.\n",
      "libcst 0.3.21 requires pyyaml>=5.2, but you have pyyaml 5.1 which is incompatible.\n",
      "bokeh 2.4.0 requires typing_extensions>=3.10.0, but you have typing-extensions 3.7.4.3 which is incompatible.\n",
      "black 21.9b0 requires typing-extensions>=3.10.0.0, but you have typing-extensions 3.7.4.3 which is incompatible.\n",
      "awscli 1.19.106 requires s3transfer<0.5.0,>=0.4.0, but you have s3transfer 0.5.0 which is incompatible.\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "729f68e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-09 21:42:05.095224: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-10-09 21:42:05.095275: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "pd.options.display.max_rows = 999\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import activations, optimizers, losses\n",
    "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
    "import pickle\n",
    "\n",
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7a35d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [\n",
    "     'Great customer service! The food was delicious! Definitely a come again.',\n",
    "     'The VEGAN options are super fire!!! And the plates come in big portions. Very pleased with this spot, I\\'ll definitely be ordering again',\n",
    "     'Come on, this place is family owned and operated, they are super friendly, the tacos are bomb.',\n",
    "     'This is such a great restaurant. Multiple times during days that we don\\'t want to cook, we\\'ve done takeout here and it\\'s been amazing. It\\'s fast and delicious.',\n",
    "     'Staff is really nice. Food is way better than average. Good cost benefit.',\n",
    "     'pricing for this, while relatively inexpensive for a Las Vegas attraction, is completely over the top.',\n",
    "     'At such a *fine* institution, I find the lack of knowledge and respect for the art appalling',\n",
    "     'If I could give one star I would...I walked out before my food arrived the customer service was horrible!',\n",
    "     'Wow the slowest drive thru I\\'ve ever been at WOWWWW. Horrible I won\\'t be coming back here ever again',\n",
    "     'Service: 1 out of 5 stars. They will mess up your order, not have it ready after 30 mins calling them before. Worst ran family business Ive ever seen.'\n",
    "]\n",
    "\n",
    "y = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "474903ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review: 'Great customer service! The food was delicious! Definitely a come again.'\n",
      "input ids: [101, 2307, 8013, 2326, 999, 1996, 2833, 2001, 12090, 999, 5791, 1037, 2272, 2153, 1012, 102]\n",
      "attention mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'distilbert-base-uncased'\n",
    "MAX_LEN = 20\n",
    "\n",
    "review = x[0]\n",
    "\n",
    "tkzr = DistilBertTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "inputs = tkzr(review, max_length=MAX_LEN, truncation=True, padding=True)\n",
    "\n",
    "print(f'review: \\'{review}\\'')\n",
    "print(f'input ids: {inputs[\"input_ids\"]}')\n",
    "print(f'attention mask: {inputs[\"attention_mask\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcca0339",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_encodings(x, tkzr, max_len, trucation=True, padding=True):\n",
    "    return tkzr(x, max_length=max_len, truncation=trucation, padding=padding)\n",
    "    \n",
    "encodings = construct_encodings(x, tkzr, max_len=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d30e2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-12 22:10:33.243659: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-12 22:10:33.244027: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2021-09-12 22:10:33.244040: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-09-12 22:10:33.244062: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (jupyter-stella-40dev2vec-2ecom): /proc/driver/nvidia/version does not exist\n",
      "2021-09-12 22:10:33.244301: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-12 22:10:33.244599: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n"
     ]
    }
   ],
   "source": [
    "def construct_tfdataset(encodings, y=None):\n",
    "    if y:\n",
    "        return tf.data.Dataset.from_tensor_slices((dict(encodings),y))\n",
    "    else:\n",
    "        # this case is used when making predictions on unseen samples after training\n",
    "        return tf.data.Dataset.from_tensor_slices(dict(encodings))\n",
    "    \n",
    "tfdataset = construct_tfdataset(encodings, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "548fc714",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SPLIT = 0.2\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "train_size = int(len(x) * (1-TEST_SPLIT))\n",
    "\n",
    "tfdataset = tfdataset.shuffle(len(x))\n",
    "tfdataset_train = tfdataset.take(train_size)\n",
    "tfdataset_test = tfdataset.skip(train_size)\n",
    "\n",
    "tfdataset_train = tfdataset_train.batch(BATCH_SIZE)\n",
    "tfdataset_test = tfdataset_test.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3ece69",
   "metadata": {},
   "source": [
    "## fine tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3d432ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-12 22:10:36.006811: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_transform', 'vocab_layer_norm', 'vocab_projector', 'activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier', 'dropout_19', 'classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-12 22:10:43.159486: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-09-12 22:10:43.211938: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2999995000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 9s 289ms/step - loss: 0.6869 - accuracy: 0.5500\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 1s 292ms/step - loss: 0.6949 - accuracy: 0.4833\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 1s 284ms/step - loss: 0.6263 - accuracy: 0.7667\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 1s 289ms/step - loss: 0.5616 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 1s 278ms/step - loss: 0.4782 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 1s 283ms/step - loss: 0.3332 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 1s 281ms/step - loss: 0.2443 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 1s 278ms/step - loss: 0.1890 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 1s 279ms/step - loss: 0.1391 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 1s 278ms/step - loss: 0.1004 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f80dfe3d8b0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "optimizer = optimizers.Adam(learning_rate=3e-5)\n",
    "loss = losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "model.fit(tfdataset_train, batch_size=BATCH_SIZE, epochs=N_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9422bbbf",
   "metadata": {},
   "source": [
    "## test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "897bb635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step - loss: 0.0707 - accuracy: 1.0000\n",
      "{'loss': 0.07065244019031525, 'accuracy': 1.0}\n"
     ]
    }
   ],
   "source": [
    "benchmarks = model.evaluate(tfdataset_test, return_dict=True, batch_size=BATCH_SIZE)\n",
    "print(benchmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "013ee701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13448104\n"
     ]
    }
   ],
   "source": [
    "def create_predictor(model, model_name, max_len):\n",
    "    tkzr = DistilBertTokenizer.from_pretrained(model_name)\n",
    "    def predict_proba(text):\n",
    "        x = [text]\n",
    "        encodings = construct_encodings(x, tkzr, max_len=max_len)\n",
    "        tfdataset = construct_tfdataset(encodings)\n",
    "        tfdataset = tfdataset.batch(1)\n",
    "\n",
    "        preds = model.predict(tfdataset).logits\n",
    "        preds = activations.softmax(tf.convert_to_tensor(preds)).numpy()\n",
    "        return preds[0][0]\n",
    "    \n",
    "    return predict_proba\n",
    "\n",
    "clf = create_predictor(model, MODEL_NAME, MAX_LEN)\n",
    "print(clf('this restaurant has horrible food'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3296ebd9",
   "metadata": {},
   "source": [
    "## (Optional) save model to cloud for checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffc288db",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save model locally\n",
    "model.save_pretrained('/root/model/clf')\n",
    "with open('/root/model/info.pkl', 'wb') as f:\n",
    "    pickle.dump((MODEL_NAME, MAX_LEN), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2ce4aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_cloud(local_file_name, remote_file_name):\n",
    "    import os\n",
    "    import s3fs\n",
    "    import gcsfs\n",
    "    cloud_name = remote_file_name.split('://')[0]\n",
    "    if cloud_name =='gs':\n",
    "        fs = gcsfs.GCSFileSystem(project=os.environ['GCP_PROJECT'])\n",
    "    elif cloud_name =='s3':\n",
    "        fs = s3fs.S3FileSystem()\n",
    "    else:\n",
    "        raise NameError(f'cloud name {cloud_name} unknown')\n",
    "    try:    \n",
    "        print(f'upload {local_file_name} to {remote_file_name}')\n",
    "        fs.put(local_file_name, remote_file_name, recursive=True)\n",
    "        print(\"done uploading!\")\n",
    "    except Exception as exp:\n",
    "        print(f\"upload failed: {exp}\")\n",
    "        \n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f0cc78",
   "metadata": {},
   "source": [
    "**Note: the remote_file_path is a path of a GCP or S3 bucket that connected to your Hyperplane cluster**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e9ed43a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload model to gs://pipeline_data/demo/bach_inference/model\n",
      "done uploading!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_file_path = 'model'\n",
    "remote_file_path = f\"s3://d2v-tmp/demo/bach_inference/{model_file_path}\"\n",
    "\n",
    "upload_to_cloud(model_file_path, remote_file_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
